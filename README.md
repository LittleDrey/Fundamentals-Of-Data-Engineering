# Fundamentals-Of-Data-Engineering
Consolidando Conceitos e Fundamentos do Livro 'Fundamentals-Of-Data-Engineering' em Uni√£o com Boas Pr√°ticas de Trabalho


# ‚öΩ Futebol Analytics Data Pipeline

> Um projeto de Engenharia de Dados End-to-End aplicando os conceitos fundamentais do livro *"Fundamentals of Data Engineering"* (Joe Reis & Matt Housley).

## üéØ Objetivo do Projeto
Construir um pipeline de dados robusto, escal√°vel e moderno para processar dados de futebol (jogadores, partidas, torneios, ...), traduzindo a teoria de engenharia de dados em pr√°tica real utilizando **Databricks** e **Delta Lake**.

O projeto segue a arquitetura **Medallion** (Bronze, Silver, Gold), com foco em qualidade de dados, governan√ßa e otimiza√ß√£o de armazenamento.

## üìö Fundamenta√ß√£o Te√≥rica & Decis√µes Arquiteturais

Este projeto √© guiado pelas fases do Ciclo de Vida da Engenharia de Dados. Abaixo est√£o as decis√µes t√©cnicas tomadas para cada etapa:

### 1. Ingest√£o (Ingestion)
*Baseado no Cap√≠tulo 7: Ingest√£o de Dados*

* **Padr√£o de Movimenta√ß√£o:** **Push** (Empurrar). Os arquivos s√£o enviados da origem local para o Data Lake.
* **Frequ√™ncia:** **Batch** (Lote). Como lidamos com dados hist√≥ricos de partidas e torneios, a ingest√£o em lote √© a escolha pragm√°tica, evitando a complexidade desnecess√°ria de streaming para dados que n√£o exigem lat√™ncia de milissegundos.
* **Filosofia:** "Encanamento" (Plumbing). Nesta etapa, o foco foi puramente mover os dados do ponto A (Local) para o ponto B (Staging Zone) sem transforma√ß√µes, garantindo uma c√≥pia fiel da origem.

## üõ†Ô∏è Infraestrutura e Organiza√ß√£o

### Estrutura do Data Lake (Staging Zone)
Para evitar o antipadr√£o do "Data Swamp" (P√¢ntano de Dados), a zona de aterrissagem (Staging) foi estruturada hierarquicamente para garantir governan√ßa e facilitar a leitura automatizada:

```text
staging_zone/ (Volume Databricks)
‚îú‚îÄ‚îÄ futebol_db/          <-- Sistema de Origem (Source System)
‚îÇ   ‚îú‚îÄ‚îÄ torneios/        <-- Entidade de Neg√≥cio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csv/         <-- Formato do Arquivo
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _torneios.csv
‚îÇ   ‚îú‚îÄ‚îÄ jogadores/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _jogadores.json
‚îÇ   ‚îî‚îÄ‚îÄ partidas/
‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ csv/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _partidas.csv
```
---

## üöß Desafios de Engenharia e Solu√ß√µes (War Stories)

Durante a fase de Ingest√£o, enfrentei limita√ß√µes de infraestrutura e problemas de qualidade de dados na origem. Abaixo documentei como superar cada barreira, alinhando com os fundamentos do livro.

### 1. Restri√ß√£o de Infraestrutura: Streaming em Cluster Compartilhado
* **O Problema:** A inten√ß√£o original era utilizar o **Databricks Auto Loader** (`cloud_files`) em modo Streaming para garantir checkpoints autom√°ticos. Por√©m, o ambiente utilizado (Databricks Community Edition/Free Edition) opera em **Shared Clusters**, que bloqueiam permiss√µes de baixo n√≠vel necess√°rias para a gest√£o de filas de arquivos do Auto Loader.
* **Erro Encontrado:** `[UNSUPPORTED_STREAMING_SOURCE_PERMISSION_ENFORCED]`
* **A Solu√ß√£o (Trade-off):** A arquitetura foi adaptada para **Batch Read**, mantendo o c√≥digo modular. Aceitei a perda tempor√°ria do checkpoint autom√°tico em favor da execu√ß√£o funcional, entendendo que em um ambiente Enterprise (Single User Cluster), a chave `use_stream=True` reativaria a capacidade de streaming sem refatora√ß√£o de c√≥digo.

### 2. Serializa√ß√£o e Encoding (UTF-16)
* **O Problema:** A ingest√£o inicial dos arquivos CSV resultou em dados corrompidos ("Mojibake") e falha na identifica√ß√£o de colunas (todas lidas como `_c0`), pois o Spark assume `UTF-8` por padr√£o.
* **A Solu√ß√£o:** Foi implementado um tratamento espec√≠fico no Reader do Spark para for√ßar o encoding correto detectado na origem (`UTF-16`), al√©m da defini√ß√£o expl√≠cita de delimitadores e cabe√ßalhos.

### 3. Integridade do Schema (JSON Aninhado)
* **O Problema:** Arquivo JSON (Jogadores) e CSV (Eventos) continham estruturas complexas aninhadas e colunas com strings que representavam objetos JSON.
* **Decis√£o Arquitetural:** Seguindo o princ√≠pio da camada **Bronze** (preservar o dado bruto), optei por n√£o "explodir" ou limpar esses JSONs na ingest√£o. Eles foram persistidos como Strings ou Structs brutos para serem tratados na camada Silver, garantindo que falhas de parser n√£o interrompam o pipeline de ingest√£o.

---

## üîÆ Roadmap para Camada Silver (Transforma√ß√£o)

Com base no *Profiling* dos dados da camada Bronze, mapeei as seguintes necessidades de tratamento para a pr√≥xima fase:

| Tabela | Problema Identificado (Bronze) | A√ß√£o Planejada (Silver) |
| :--- | :--- | :--- |
| **Jogadores** | Coluna `height` cont√©m caracteres sujos (ex: `> 200 cm`). | Limpeza de strings (Regex) e Cast para `Integer`. |
| **Jogadores** | Dados aninhados em estruturas JSON. | Parser e `Flatten` das colunas. |
| **Eventos** | Colunas `team`, `player`, `assist` s√£o strings JSON (`{'id': 10...}`). | Uso de `from_json` para estruturar IDs e Nomes. |
| **Partidas** | Colunas de data sem padr√£o definido. | Padroniza√ß√£o para `DateType` ou `Timestamp`. |
| **Geral** | Nomes de colunas fora do padr√£o (ex: CamelCase ou com pontos `league.season`). | Renomea√ß√£o para `snake_case` (ex: `league_season`). |

---

## üèóÔ∏è Fase 2: Transforma√ß√£o Raw (Bronze) -> Silver

Esta fase focou na **limpeza, padroniza√ß√£o e estrutura√ß√£o** dos dados brutos. O objetivo foi transformar dados "ca√≥ticos" (Raw) em tabelas confi√°veis, tipadas e otimizadas para an√°lise (Silver), aplicando conceitos de *Schema Enforcement* e *Data Quality*.

### ‚öîÔ∏è War Stories: Desafios T√©cnicos & Solu√ß√µes

Durante a constru√ß√£o do pipeline, enfrentei inconsist√™ncias cr√≠ticas nos dados de origem. Abaixo, detalho os cen√°rios de "crise" e as solu√ß√µes de engenharia aplicadas.

#### 1. O Desafio do "Encoding H√≠brido" (Mojibake)
**O Problema:** A ingest√£o da tabela `partidas` falhou silenciosamente. Dados hist√≥ricos (2011-2019) foram gerados em **UTF-16LE** (padr√£o Excel legado), enquanto dados recentes (2023) chegaram em **UTF-8**.
* **Sintoma:** Ao for√ßar uma leitura √∫nica, o Spark interpretou bytes UTF-8 como UTF-16, gerando caracteres chineses (ex: `„Ñ∞„Äµ...`) na coluna de IDs. Isso √© conhecido tecnicamente como *Mojibake*.
* **Impacto:** Corrup√ß√£o total dos IDs e falha na tipagem (Integers viraram Strings).

**A Solu√ß√£o (Smart Ingestion Pattern):**
Desenvolvi uma fun√ß√£o "Sniffer" (Farejadora) que inspeciona os primeiros bytes (Magic Bytes) de cada arquivo antes da leitura total.
* **L√≥gica:** Se o arquivo inicia com `\xff\xfe` (BOM), o pipeline aplica decoder UTF-16. Caso contr√°rio, assume UTF-8.
* **Resultado:** Ingest√£o h√≠brida bem-sucedida, unificando arquivos com encodings diferentes no mesmo DataFrame via `unionByName`.

#### 2. Schema Drift & Conflito no Delta Lake
**O Problema:** Devido √† ingest√£o corrompida anterior, o Delta Lake registrou a coluna `id` como `STRING` nos metadados. Ao corrigir o encoding, os dados chegaram corretamente como `INTEGER`.
* **Erro:** `[DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'id' and 'id'.`
* **Conceito:** O Delta Lake protege a integridade do schema (Schema Enforcement), impedindo mudan√ßas bruscas de tipo.

**A Solu√ß√£o:**
Implementa√ß√£o de uma estrat√©gia de **Schema Evolution Controlada** na camada Bronze:
1.  Uso da op√ß√£o `.option("overwriteSchema", "true")` para for√ßar a atualiza√ß√£o dos metadados.
2.  Execu√ß√£o preventiva de `DROP TABLE` para limpar logs de transa√ß√£o contaminados em ambiente de desenvolvimento.

#### 3. Data Wrangling em "Fake JSONs" (Tabela Eventos)
**O Problema:** A tabela de eventos continha colunas (`player`, `team`, `time`, `assist`) que pareciam JSON, mas eram representa√ß√µes de dicion√°rios Python (aspas simples `'` e `None` em vez de `null`). O parser nativo do Spark falhava.
* **A Solu√ß√£o:** Pipeline de higieniza√ß√£o via Regex antes do parsing.
    * Substitui√ß√£o de aspas simples por duplas.
    * Tratamento de literais `None` para `null`.
    * Aplica√ß√£o de `from_json` com Schema expl√≠cito (DDL) para garantir tipagem forte.
 
#### 4. Tratamento de JSONs Verdadeiros "Struct" (Tabela Jogadores)
**O Problema:** A tabela de jogadores continha uma coluna (`birth`) apresentada como JSON, utilizando aspas duplas `"` e `Null`. E dados incosistentes nas colunas `height` e `weight`.
* **A Solu√ß√£o:** Parser nativo do Spark juntamente com remo√ß√£o de caracteres n√£o num√©ricos.
  *  Regex Cleanning e Flattening.

#### 5. Regras de Neg√≥cio e Corre√ß√£o de Dom√≠nio
Para garantir a qualidade anal√≠tica na camada Silver, aplicamos regras de neg√≥cio corretivas:
* **Futebol Domain Check:** Na tabela `eventos`, detectei minutos negativos (ex: `-5`). Apliquei a fun√ß√£o `abs()` (valor absoluto) assumindo erro de digita√ß√£o na origem.
* **Entity Resolution:** Na tabela `times`, times brasileiros estavam marcados incorretamente como `national = False`. Apliquei regra condicional: `WHEN country = 'Brazil' THEN is_national = True`.
* **Tratamento de Strings Num√©ricas:** A coluna `score.fulltime.away` continha n√∫meros formatados como string com ponto flutuante ("2.0"). Apliquei cast (String -> Int) ou regex para limpeza.

#### 6. Observabilidade e o Falso Positivo (Databricks Lakehouse Monitoring)
**O Problema:** A implementa√ß√£o do monitoramento de qualidade estat√≠stica via Databricks Unity Catalog apontou aus√™ncia de duplicatas em tabelas que, ao serem consultadas via PySpark, possu√≠am chaves prim√°rias duplicadas.
* **A Causa:** Desalinhamento entre a "Verdade do Log" e a "Verdade do Momento". O Monitoramento operava como um Job agendado (foto do passado), enquanto o cluster consultava o estado atual corrompido por novas cargas.
* **A Solu√ß√£o:** Separa√ß√£o estrita de schemas. Foi criado o schema *Sidecar* `workspace_project.data_governance` exclusivo para isolar as tabelas de m√©tricas (`_profile_metrics` e `_drift_metrics`), garantindo que ferramentas de BI n√£o realizem scans acidentais em metadados. Monitores foram ajustados: *Snapshot* para tabelas est√°ticas (Dimens√µes) e *Time Series* para eventos (Fatos).

#### 7. O Paradigma da Deduplica√ß√£o: Window Function vs dropDuplicates
**O Problema:** Identifica√ß√£o de linhas duplicadas na tabela `jogadores` e `torneios` na camada Silver, gerando distor√ß√£o anal√≠tica. O uso do m√©todo nativo `dropDuplicates()` do Spark traria n√£o-determinismo (risco de manter a vers√£o desatualizada de um dado).
* **A Solu√ß√£o (Jogadores - SCD Type 1):** Implementa√ß√£o de uma fun√ß√£o modular utilizando `Window Function` (ordenando por `ingestion_date` DESC) para garantir a extra√ß√£o do *Golden Record* (registro mais recente). Ap√≥s a limpeza em mem√≥ria, os dados s√£o persistidos no Delta Lake utilizando a instru√ß√£o transacional `MERGE INTO` (Upsert), garantindo atualiza√ß√£o de registros existentes e inser√ß√£o de novos.
* **A Solu√ß√£o (Torneios - Chave Composta):** Preven√ß√£o de perda de dados hist√≥ricos (SCD Tipo 2). A fun√ß√£o de deduplica√ß√£o foi parametrizada para atuar sobre uma Chave Prim√°ria Composta (`tournament_id` + `season_year`), preservando o hist√≥rico de todas as temporadas de um mesmo campeonato.

---

### üõ†Ô∏è Decis√µes de Arquitetura (Design Patterns)

1.  **Schema Contract (.select vs .withColumn):**
    * Adotamos o uso estrito de `.select()` na transi√ß√£o para Silver.
    * *Por que?* Isso funciona como um "Contrato de Dados". Apenas colunas explicitamente listadas e tipadas entram na Silver. Colunas "lixo" ou tempor√°rias da Bronze s√£o descartadas automaticamente, garantindo uma tabela limpa.

2.  **Chaves Substitutas (Surrogate Keys):**
    * Geramos chaves internas (`sk`) usando `monotonically_increasing_id()`.
    * *Motivo:* Desacoplar o Data Lake dos IDs do sistema de origem, protegendo contra duplicidade ou mudan√ßas de chaves no legado.

3.  **Linhagem de Dados (Data Lineage):**
    * Todas as tabelas Silver mant√™m as colunas `source_file` e `ingestion_date`.
    * *Benef√≠cio:* Rastreabilidade total. Se um dado estiver errado no Dashboard, sabemos exatamente qual arquivo CSV/JSON originou o erro e quando foi processado.

4.  **FinOps & Otimiza√ß√£o:**
    * Convers√£o de tipos `BigInt` (padr√£o Spark) para `Integer` onde o dom√≠nio de dados permite, reduzindo o tamanho do armazenamento e custo de I/O.
    * Armazenamento em formato **Delta Lake** (Parquet comprimido com Snappy) para leitura colunar otimizada.

5.  **Data Quality Constraints (Fail-Fast):**
    * Aplicamos restri√ß√µes f√≠sicas no banco de dados atrav√©s do Unity Catalog (ex: `ALTER TABLE ... ADD CONSTRAINT pk_jogadores PRIMARY KEY (player_id)`).
    * *Benef√≠cio:* Mudan√ßa de uma governan√ßa reativa (apagar duplicatas no c√≥digo) para uma governan√ßa ativa (o banco de dados rejeita transa√ß√µes que ferem a integridade relacional, economizando processamento e evitando falhas silenciosas).    

---

### ‚öôÔ∏è Architecture Decision Record (ADR): Estrat√©gia de Particionamento F√≠sico

**Contexto:**
Durante o design da camada Silver e Gold, avaliei a necessidade de particionar fisicamente as tabelas do Data Lake (ex: `PARTITION BY season_year` ou `match_date`), uma pr√°tica comum para acelerar a leitura de dados via *Partition Pruning*.

**Decis√£o:**
Optei por **N√ÉO PARTICIONAR** fisicamente as tabelas deste projeto (Times, Jogadores, Partidas, Eventos, Torneios e Est√°dios).

**Justificativa T√©cnica (O Problema dos Pequenos Arquivos):**
Em Engenharia de Dados, o particionamento √© recomendado exclusivamente para tabelas massivas onde cada parti√ß√£o f√≠sica resulte em diret√≥rios com, no m√≠nimo, **1 GB a 2 GB de dados**. 
O dataset do dom√≠nio de Futebol √© de baixa volumetria (megabytes por temporada). Se fosse aplicado o particionamento por ano:
1. Seria criado micro-arquivos (alguns KBs) para cada temporada.
2. Geraria um *Metadata Overhead* massivo: o Apache Spark gastaria muito mais tempo e processamento listando diret√≥rios recursivamente do que efetivamente lendo os dados.
3. Degradaria a performance de leitura global (Small Files Problem).

**Estrat√©gia Adotada (Modern Data Stack):**
Para garantir a otimiza√ß√£o das consultas anal√≠ticas na camada Gold sem incorrer no erro de *over-partitioning*, utilizarei os recursos nativos de indexa√ß√£o do Delta Lake.
* Execu√ß√£o di√°ria do comando `OPTIMIZE` para consolidar os dados em arquivos Parquet de tamanho ideal.
* Aplica√ß√£o de `ZORDER BY (match_id, match_season_year)` nas tabelas de Fato (`partidas` e `eventos`) para co-localizar dados relacionados fisicamente, permitindo *Data Skipping* din√¢mico em consultas de BI sem a necessidade de pastas f√≠sicas.
