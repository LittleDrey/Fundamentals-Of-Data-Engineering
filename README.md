# Fundamentals-Of-Data-Engineering
Consolidando Conceitos e Fundamentos do Livro 'Fundamentals-Of-Data-Engineering' em Uni√£o com Boas Pr√°ticas de Trabalho


# ‚öΩ Futebol Analytics Data Pipeline

> Um projeto de Engenharia de Dados End-to-End aplicando os conceitos fundamentais do livro *"Fundamentals of Data Engineering"* (Joe Reis & Matt Housley).

## üéØ Objetivo do Projeto
Construir um pipeline de dados robusto, escal√°vel e moderno para processar dados de futebol (jogadores, partidas, torneios, ...), traduzindo a teoria de engenharia de dados em pr√°tica real utilizando **Databricks** e **Delta Lake**.

O projeto segue a arquitetura **Medallion** (Bronze, Silver, Gold), com foco em qualidade de dados, governan√ßa e otimiza√ß√£o de armazenamento.

## üìö Fundamenta√ß√£o Te√≥rica & Decis√µes Arquiteturais

Este projeto √© guiado pelas fases do Ciclo de Vida da Engenharia de Dados. Abaixo est√£o as decis√µes t√©cnicas tomadas para cada etapa:

### 1. Ingest√£o (Ingestion)
*Baseado no Cap√≠tulo 7: Ingest√£o de Dados*

* **Padr√£o de Movimenta√ß√£o:** **Push** (Empurrar). Os arquivos s√£o enviados da origem local para o Data Lake.
* **Frequ√™ncia:** **Batch** (Lote). Como lidamos com dados hist√≥ricos de partidas e torneios, a ingest√£o em lote √© a escolha pragm√°tica, evitando a complexidade desnecess√°ria de streaming para dados que n√£o exigem lat√™ncia de milissegundos.
* **Filosofia:** "Encanamento" (Plumbing). Nesta etapa, o foco foi puramente mover os dados do ponto A (Local) para o ponto B (Staging Zone) sem transforma√ß√µes, garantindo uma c√≥pia fiel da origem.

## üõ†Ô∏è Infraestrutura e Organiza√ß√£o

### Estrutura do Data Lake (Staging Zone)
Para evitar o antipadr√£o do "Data Swamp" (P√¢ntano de Dados), a zona de aterrissagem (Staging) foi estruturada hierarquicamente para garantir governan√ßa e facilitar a leitura automatizada:

```text
staging_zone/ (Volume Databricks)
‚îú‚îÄ‚îÄ futebol_db/          <-- Sistema de Origem (Source System)
‚îÇ   ‚îú‚îÄ‚îÄ torneios/        <-- Entidade de Neg√≥cio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csv/         <-- Formato do Arquivo
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _torneios.csv
‚îÇ   ‚îú‚îÄ‚îÄ jogadores/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _jogadores.json
‚îÇ   ‚îî‚îÄ‚îÄ partidas/
‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ csv/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _partidas.csv
```
---

## üöß Desafios de Engenharia e Solu√ß√µes (War Stories)

Durante a fase de Ingest√£o, enfrentei limita√ß√µes de infraestrutura e problemas de qualidade de dados na origem. Abaixo documentei como superar cada barreira, alinhando com os fundamentos do livro.

### 1. Restri√ß√£o de Infraestrutura: Streaming em Cluster Compartilhado
* **O Problema:** A inten√ß√£o original era utilizar o **Databricks Auto Loader** (`cloud_files`) em modo Streaming para garantir checkpoints autom√°ticos. Por√©m, o ambiente utilizado (Databricks Community Edition/Free Edition) opera em **Shared Clusters**, que bloqueiam permiss√µes de baixo n√≠vel necess√°rias para a gest√£o de filas de arquivos do Auto Loader.
* **Erro Encontrado:** `[UNSUPPORTED_STREAMING_SOURCE_PERMISSION_ENFORCED]`
* **A Solu√ß√£o (Trade-off):** A arquitetura foi adaptada para **Batch Read**, mantendo o c√≥digo modular. Aceitei a perda tempor√°ria do checkpoint autom√°tico em favor da execu√ß√£o funcional, entendendo que em um ambiente Enterprise (Single User Cluster), a chave `use_stream=True` reativaria a capacidade de streaming sem refatora√ß√£o de c√≥digo.

### 2. Serializa√ß√£o e Encoding (UTF-16)
* **O Problema:** A ingest√£o inicial dos arquivos CSV resultou em dados corrompidos ("Mojibake") e falha na identifica√ß√£o de colunas (todas lidas como `_c0`), pois o Spark assume `UTF-8` por padr√£o.
* **A Solu√ß√£o:** Foi implementado um tratamento espec√≠fico no Reader do Spark para for√ßar o encoding correto detectado na origem (`UTF-16`), al√©m da defini√ß√£o expl√≠cita de delimitadores e cabe√ßalhos.

### 3. Integridade do Schema (JSON Aninhado)
* **O Problema:** Arquivo JSON (Jogadores) e CSV (Eventos) continham estruturas complexas aninhadas e colunas com strings que representavam objetos JSON.
* **Decis√£o Arquitetural:** Seguindo o princ√≠pio da camada **Bronze** (preservar o dado bruto), optei por n√£o "explodir" ou limpar esses JSONs na ingest√£o. Eles foram persistidos como Strings ou Structs brutos para serem tratados na camada Silver, garantindo que falhas de parser n√£o interrompam o pipeline de ingest√£o.

---

## üîÆ Roadmap para Camada Silver (Transforma√ß√£o)

Com base no *Profiling* dos dados da camada Bronze, mapeei as seguintes necessidades de tratamento para a pr√≥xima fase:

| Tabela | Problema Identificado (Bronze) | A√ß√£o Planejada (Silver) |
| :--- | :--- | :--- |
| **Jogadores** | Coluna `height` cont√©m caracteres sujos (ex: `> 200 cm`). | Limpeza de strings (Regex) e Cast para `Integer`. |
| **Jogadores** | Dados aninhados em estruturas JSON. | Parser e `Flatten` das colunas. |
| **Eventos** | Colunas `team`, `player`, `assist` s√£o strings JSON (`{'id': 10...}`). | Uso de `from_json` para estruturar IDs e Nomes. |
| **Partidas** | Colunas de data sem padr√£o definido. | Padroniza√ß√£o para `DateType` ou `Timestamp`. |
| **Geral** | Nomes de colunas fora do padr√£o (ex: CamelCase ou com pontos `league.season`). | Renomea√ß√£o para `snake_case` (ex: `league_season`). |
